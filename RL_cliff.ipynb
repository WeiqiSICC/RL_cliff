{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning 2021-22: Second Group Assignment\n",
    "\n",
    "**Deadline**: Mon 14 March 2022, 23:59\n",
    "\n",
    "|Nr|**Name**|**Student ID**|**Email**|\n",
    "|--|--------|--------------|---------|\n",
    "|1.|Zhen Fang|  12542636|   fangzhen0506@yahoo.com       |\n",
    "|2.| Weiqi Mao       | 12542644             |   12542644@uva.nl|\n",
    "|3.|        |              |         |\n",
    "\n",
    "**Declaration of Originality**\n",
    "\n",
    "We whose names are given under 1., 2., and 3. above declare that:\n",
    "\n",
    "1. These solutions are solely our own work.\n",
    "2. We have not made (part of) these solutions available to any other student.\n",
    "3. We shall not engage in any other activities that will dishonestly improve my results or dishonestly improve or hurt the results of others.\n",
    "\n",
    "## Instructions for completing and submitting the assignment\n",
    "Please pay attention to the following instructions:\n",
    "\n",
    "1. Please follow carefully the steps outlined in the assignment. If you cannot solve an exercise and this hinders continuing with subsequent exercises, try to find a way to work around it and give a clear explanation for the solution you have chosen.\n",
    "2. Submit your work in the form of a Jupyter notebook and a html file (see point 6 below) via Canvas, before the deadline. Your notebook should not give errors when executed with `Run All`.\n",
    "3. Most of your answers will consist of code. Make sure your code is well structured, efficient and provided with comments where needed. These aspects will be taken into account in the grading of your work.\n",
    "4. Sometimes you are asked to answer some (open) questions. Please type your answer in the given Markdown cells and use your own words in answering those questions.\n",
    "5. You are allowed to work on the assignment in groups of no more than 3 students and to submit together.\n",
    "6. Save the complete work with all outputs in an HTML file: \"File\"->\"Download as\" -> \"HTML (.html)\" in Jupyter Notebook. Upload this HTML file to canvas as a supplement. You must submit BOTH notebook and HTML files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Problem\n",
    "\n",
    "In this assignment, you will solve the taxi problem for the following map:\n",
    "\n",
    "````\n",
    "        +---------+\n",
    "        |R: | : :G|\n",
    "        | : | : : |\n",
    "        | : : : : |\n",
    "        | | : | : |\n",
    "        |Y| : |B: |\n",
    "        +---------+\n",
    "````\n",
    "\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends. The rewards are:\n",
    "\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing \"pickup\" and \"drop-off\" actions illegally.\n",
    "\n",
    "If a navigation action would cause the taxi to hit a wall (solid borders in the map), the action is a no-op, and there is only the usual reward of −1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "GroupNumber=26 # Input your group number Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the ``TaxiEnv`` class from env_taxi.py and create an instance of the Taxi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_taxi import TaxiEnv\n",
    "env=TaxiEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we reset the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns four state variables:\n",
    "\n",
    "- Row index of the taxi (starting from 0 in Python)\n",
    "- Colum index of the taxi (starting from 0 in Python)\n",
    "- Passenger location (0-4): 0=R, 1=G, 2=Y, 3=B, 4=in taxi\n",
    "- Destination location (0-3): same encoding rule as that for passenger location but excluding the \"in taxi\" option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the ``describe_state`` method of the Taxi instance to display the state variables without location encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Taxi Location': [2, 3], 'Passenger Location': 'Y', 'Destination Index': 'G'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.describe_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the ``render`` method to visualize the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 discrete deterministic actions:\n",
    "- 'South': move south\n",
    "- 'West': move north\n",
    "- 'East': move east\n",
    "- 'West': move west\n",
    "- 'Pickup': pickup passenger\n",
    "- 'Dropoff': drop off passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['South', 'North', 'East', 'West', 'Pickup', 'Dropoff']\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move one step to west:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 2, 1), -1, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step('West')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a 3-tuple: the new state (a list of 4 numbers), reward, and whether the episode is ended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the new state. Note that the last action is shown at the bottom as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Complete the codes below to generate _one_ episode for the taxi driver who:\n",
    "- Picks up the passenger when the taxi is at the location of the passenger when they are not yet at the destination;\n",
    "- Drops off the passenger in the taxi when reaching the destination;\n",
    "- Moves randomly with equal probabilities when finding the passenger or when the passenger is in the taxi (but not yet at destination). \n",
    "\n",
    "Then print the sum of rewards for the taxi driver in your episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "{'Taxi Location': [2, 3], 'Passenger Location': 'Y', 'Destination Index': 'B'}\n"
     ]
    }
   ],
   "source": [
    "state=env.reset(271) # The number 271 fixes the initial state. Do not change it.\n",
    "env.render()\n",
    "print(env.describe_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us program the policy function for the taxi driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_policy(state):\n",
    "    ## Start Coding Here\n",
    "    \n",
    "    if state[2]!=4:\n",
    "        if state[0:2]==env.locs[state[2]] and state[0:2]!=env.locs[state[3]]:\n",
    "            action=env.action_space[4] #Pickup\n",
    "        else:\n",
    "            action=np.random.choice(env.action_space[0:4])#randomly move\n",
    "    if state[2]==4:\n",
    "        if state[0:2]==env.locs[state[3]]:\n",
    "            action=env.action_space[5]# Dropoff\n",
    "        else:\n",
    "            action=np.random.choice(env.action_space[0:4])#randomly move\n",
    "            \n",
    "    ## End Coding Here\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function simulate_episode in module env_taxi:\n",
      "\n",
      "simulate_episode(env, policy, max_iter=inf)\n",
      "    Simulate a episode following a given policy\n",
      "    @param env: game environment\n",
      "    @param policy: policy (a dictionary or a function)\n",
      "    @return: resulting states, actions and rewards for the entire episode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the simulate_episode function from env_taxi.py to help you simulate episode(s)\n",
    "from env_taxi import simulate_episode\n",
    "help(simulate_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return for this episode: -426\n"
     ]
    }
   ],
   "source": [
    "# Set the seed to make results reproducible\n",
    "np.random.seed(GroupNumber)\n",
    "\n",
    "states, actions, rewards = simulate_episode(env,naive_policy)\n",
    "\n",
    "# Save the sum of rewards in this variable G\n",
    "G=np.sum(rewards)\n",
    "\n",
    "\n",
    "# Print the return for this episode\n",
    "print(\"Return for this episode:\", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Following the steps below to investigate whether we can solve the optimal policy by using Monte Carlo Exploring Starts. If possible, complete the codes and save the optimal (greedy) policy as a dictionary/defaultdict object. Otherwise, comment on the issues you encounter. \n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function simulate_episode_ES in module env_taxi:\n",
      "\n",
      "simulate_episode_ES(env, policy, max_iter=inf)\n",
      "    Simulate a episode with exploring starts\n",
      "    @param env: game environment\n",
      "    @param policy: policy (a dictionary or a function)\n",
      "    @param max_iter: maximum number of time steps\n",
      "    @return: resulting states, actions and rewards for the entire episode\n",
      "\n",
      "[(1, 2, 2, 3), (2, 2, 2, 3), (2, 1, 2, 3), (3, 1, 2, 3), (2, 1, 2, 3), (2, 0, 2, 3), (3, 0, 2, 3), (2, 0, 2, 3), (2, 1, 2, 3), (2, 2, 2, 3), (3, 2, 2, 3), (2, 2, 2, 3), (3, 2, 2, 3), (4, 2, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (3, 1, 2, 3), (3, 1, 2, 3), (3, 2, 2, 3), (3, 2, 2, 3), (3, 1, 2, 3), (3, 1, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (3, 1, 2, 3), (4, 1, 2, 3), (3, 1, 2, 3), (3, 2, 2, 3), (2, 2, 2, 3), (1, 2, 2, 3), (1, 3, 2, 3), (1, 2, 2, 3), (1, 3, 2, 3), (1, 2, 2, 3), (2, 2, 2, 3), (3, 2, 2, 3), (4, 2, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (4, 1, 2, 3), (4, 2, 2, 3), (4, 1, 2, 3), (3, 1, 2, 3), (3, 2, 2, 3), (3, 2, 2, 3), (2, 2, 2, 3), (2, 3, 2, 3), (1, 3, 2, 3), (0, 3, 2, 3), (0, 3, 2, 3), (1, 3, 2, 3), (1, 4, 2, 3), (0, 4, 2, 3), (0, 4, 2, 3), (0, 4, 2, 3), (0, 4, 2, 3), (0, 4, 2, 3), (1, 4, 2, 3), (1, 4, 2, 3), (1, 4, 2, 3), (1, 4, 2, 3), (0, 4, 2, 3), (0, 3, 2, 3), (0, 4, 2, 3), (0, 4, 2, 3), (0, 3, 2, 3), (0, 2, 2, 3), (0, 2, 2, 3), (0, 2, 2, 3), (1, 2, 2, 3), (2, 2, 2, 3), (3, 2, 2, 3), (4, 2, 2, 3), (4, 2, 2, 3), (4, 2, 2, 3), (4, 2, 2, 3), (4, 2, 2, 3), (3, 2, 2, 3), (3, 1, 2, 3), (2, 1, 2, 3), (1, 1, 2, 3), (1, 1, 2, 3), (0, 1, 2, 3), (1, 1, 2, 3), (1, 0, 2, 3), (1, 0, 2, 3), (2, 0, 2, 3), (2, 0, 2, 3), (2, 0, 2, 3), (2, 0, 2, 3), (2, 1, 2, 3), (2, 0, 2, 3), (3, 0, 2, 3), (4, 0, 2, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (3, 0, 4, 3), (2, 0, 4, 3), (2, 0, 4, 3), (2, 1, 4, 3), (3, 1, 4, 3), (3, 1, 4, 3), (2, 1, 4, 3), (3, 1, 4, 3), (3, 1, 4, 3), (3, 2, 4, 3), (2, 2, 4, 3), (3, 2, 4, 3), (3, 1, 4, 3), (4, 1, 4, 3), (4, 1, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (3, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 1, 4, 3), (4, 1, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 1, 4, 3), (3, 1, 4, 3), (2, 1, 4, 3), (3, 1, 4, 3), (3, 2, 4, 3), (3, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (4, 2, 4, 3), (3, 2, 4, 3), (4, 2, 4, 3), (4, 1, 4, 3), (4, 1, 4, 3), (4, 2, 4, 3), (3, 2, 4, 3), (2, 2, 4, 3), (2, 1, 4, 3), (1, 1, 4, 3), (1, 1, 4, 3), (1, 1, 4, 3), (1, 0, 4, 3), (1, 0, 4, 3), (2, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (3, 0, 4, 3), (3, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (3, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (4, 0, 4, 3), (3, 0, 4, 3), (2, 0, 4, 3), (1, 0, 4, 3), (0, 0, 4, 3), (0, 0, 4, 3), (1, 0, 4, 3), (2, 0, 4, 3), (3, 0, 4, 3), (2, 0, 4, 3), (3, 0, 4, 3), (2, 0, 4, 3), (2, 0, 4, 3)] ['South', 'West', 'South', 'North', 'West', 'South', 'North', 'East', 'East', 'South', 'North', 'South', 'South', 'West', 'West', 'North', 'West', 'East', 'East', 'West', 'West', 'South', 'South', 'South', 'West', 'North', 'South', 'North', 'East', 'North', 'North', 'East', 'West', 'East', 'West', 'South', 'South', 'South', 'West', 'South', 'South', 'South', 'East', 'West', 'North', 'East', 'East', 'North', 'East', 'North', 'North', 'North', 'South', 'East', 'North', 'North', 'North', 'East', 'North', 'South', 'East', 'East', 'East', 'North', 'West', 'East', 'East', 'West', 'West', 'North', 'North', 'South', 'South', 'South', 'South', 'East', 'East', 'South', 'South', 'North', 'West', 'North', 'North', 'East', 'North', 'South', 'West', 'West', 'South', 'West', 'West', 'West', 'East', 'West', 'South', 'South', 'Pickup', 'East', 'West', 'South', 'East', 'West', 'South', 'South', 'West', 'West', 'East', 'West', 'South', 'South', 'North', 'North', 'West', 'East', 'South', 'West', 'North', 'South', 'West', 'East', 'North', 'South', 'West', 'South', 'South', 'East', 'South', 'East', 'South', 'North', 'South', 'South', 'East', 'East', 'South', 'West', 'West', 'East', 'East', 'South', 'East', 'East', 'West', 'North', 'North', 'South', 'East', 'East', 'South', 'East', 'South', 'East', 'North', 'South', 'West', 'West', 'East', 'North', 'North', 'West', 'North', 'East', 'East', 'West', 'West', 'South', 'South', 'East', 'East', 'West', 'East', 'East', 'South', 'West', 'North', 'East', 'South', 'East', 'West', 'East', 'East', 'South', 'West', 'East', 'North', 'South', 'South', 'East', 'North', 'North', 'North', 'North', 'West', 'South', 'South', 'South', 'North', 'South', 'North', 'West'] [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# We import the simulate_episode_ES function from env_taxi.py to simulate episodes with exploring starts\n",
    "from env_taxi import simulate_episode_ES\n",
    "help(simulate_episode_ES)\n",
    "\n",
    "\n",
    "# Let us try to simulate one episode with exploring start for the naive policy from Exercise 1\n",
    "# Print the states, actions and rewards for an episode (the first 200 steps only)\n",
    "np.random.seed(GroupNumber)\n",
    "states, actions, rewards=simulate_episode_ES(env, naive_policy, 200)\n",
    "print(states, actions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can solve the optimal policy by using Monte Carlo Exploring Starts, write down your codes in the next cell and save the optimal (greedy) policy in the dictionary object ``pi_ES`` initialized below. Use no more than 50000 episodes. We will import the ``performance_evaluation`` function from env_taxi.py to evaluate your policy in new episodes (that terminate within 1000 time steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [11:33<00:00, 72.11it/s] \n",
      "100%|██████████| 10000/10000 [04:26<00:00, 37.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: -9993.350868100544\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.6141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save your greedy actions as the values of this dictionary, whose keys are the states\n",
    "pi_ES= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here \n",
    "\n",
    "from tqdm import tqdm\n",
    "G_sum = defaultdict(float)\n",
    "N = defaultdict(int)\n",
    "Q = defaultdict(lambda: dict(zip(env.action_space, np.empty(6))))\n",
    "for episode in tqdm(range(50000)):\n",
    "    \n",
    "\n",
    "    states_t, actions_t, rewards_t = simulate_episode_ES(env,naive_policy)\n",
    "        \n",
    "    return_t = 0\n",
    "    G = {}\n",
    "    for state_t, action_t, reward_t in zip(states_t[::-1][1:], actions_t[::-1], rewards_t[::-1]):\n",
    "        return_t =return_t + reward_t\n",
    "        G[(state_t, action_t)] = return_t\n",
    "        \n",
    "    for state_action, return_t in G.items():\n",
    "            state, action = state_action\n",
    "            G_sum[state_action] += return_t\n",
    "            N[state_action] += 1\n",
    "            Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "            pi_ES[state]=max(Q[state],key=Q[state].get)  #For each state, keep the action with the average largest value. \n",
    "\n",
    "\n",
    "## End Coding Here\n",
    "\n",
    "\n",
    "\n",
    "# We import the performance_evaluation function from env_taxi.py to evaluate your optimal policy\n",
    "from env_taxi import performance_evaluation\n",
    "performance_evaluation(env,pi_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot solve the optimal policy by using Monte Carlo Exploring Starts, explain the issue(s) and why they cannot be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Using this policy，most likely an episode cannot terminate within 1000 steps， because the first action is a random choice，the reward of the first action could also influence the average Q,and also because \"-1 per step unless other reward is triggered\",then it will take much more time for action value to converge, and to determine which action is the best choice for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:  Find the optimal policy by using Q-learning with 50000 episodes and the exploration probability $\\varepsilon=0.1$. Try two different values of the step-size parameter $\\alpha=0.4$ and $\\alpha=0.1$. Compare their performance in 10000 new episodes and comment on the similarities or/and differences. \n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:48<00:00, 462.62it/s]\n",
      "100%|██████████| 50000/50000 [01:57<00:00, 425.03it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here: Find the optimal policy by using Q-learning with 50000 episodes\n",
    "\n",
    "def gen_epsilon_greedy_policy(action_space, Q, epsilon):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        greedy_action=max(Q[state],key=Q[state].get)  #choose the action with the largest value.\n",
    "        random_action=np.random.choice(action_space)  #randomly choose among all the actions.\n",
    "        return np.random.choice([random_action,greedy_action],p=[epsilon,1-epsilon]) \n",
    "    return epsilon_greedy_policy\n",
    "def Q_learning(env, gamma, n_episode, alpha, epsilon):\n",
    "    length_episode = np.zeros(n_episode) \n",
    "    sum_reward_episode = np.zeros(n_episode)\n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.zeros(len(env.action_space)))))\n",
    "    epsilon_greedy_policy = gen_epsilon_greedy_policy(env.action_space, Q, epsilon)\n",
    "    pi= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        while not is_done:\n",
    "            next_state, reward, is_done = env.step(action)\n",
    "            next_action = epsilon_greedy_policy(state)\n",
    "           \n",
    "            v= max(Q[next_state].values())\n",
    "            \n",
    "            td_delta = reward + gamma * v - Q[state][action]\n",
    "            \n",
    "            Q[state][action] += alpha * td_delta\n",
    "            # Updte Target Policy\n",
    "            pi[state]=max(Q[state],key=Q[state].get)\n",
    "            \n",
    "            length_episode[episode] += 1\n",
    "            sum_reward_episode[episode] += reward\n",
    "            if is_done:\n",
    "                break     \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return pi\n",
    "# Save the policy for alpha=0.4 as a Python dictionary/defaultdic object pi_qlearning_1\n",
    "# Save the policy for alpha=0.1 as a Python dictionary/defaultdic object pi_qlearning_2\n",
    "\n",
    "n_episode=50000\n",
    "gamma=1\n",
    "pi_qlearning_1=Q_learning(env, gamma, n_episode, alpha=0.4, epsilon=0.1)\n",
    "pi_qlearning_2=Q_learning(env, gamma, n_episode, alpha=0.1, epsilon=0.1)\n",
    "\n",
    "## End Coding Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 208/10000 [00:00<00:04, 2073.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  Q-learning, alpha=0.4  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2224.92it/s]\n",
      "  5%|▍         | 453/10000 [00:00<00:04, 2258.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 7.8886\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n",
      "--------  Q-learning, alpha=0.1  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2228.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 7.8886\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare their performance in 10000 new episodes: Do you find a big difference?\n",
    "from env_taxi import performance_evaluation\n",
    "\n",
    "print('--------  Q-learning, alpha=0.4  --------')\n",
    "performance_evaluation(env,pi_qlearning_1)\n",
    "\n",
    "print('--------  Q-learning, alpha=0.1  --------')\n",
    "performance_evaluation(env,pi_qlearning_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commment on the similarities or/and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:Using either of these two policies could an  episode terminate within 1000 steps, the sum of rewards per episode are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Follow the steps below to find the optimal $\\varepsilon$-soft policy by using Sarsa with 50000 episodes and the exploration probability $\\varepsilon=0.1$. Try two different values of the step-size parameter $\\alpha=0.4$ and $\\alpha=0.1$. Compare their performance, and comment on the similarties or/and differences with that of Q-learning.\n",
    "\n",
    "Follow the instructions below to complete the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:04<00:00, 774.33it/s]\n",
      "100%|██████████| 50000/50000 [01:01<00:00, 816.10it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here\n",
    "def gen_epsilon_greedy_policy(action_space, Q, epsilon):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        greedy_action=max(Q[state],key=Q[state].get)\n",
    "        random_action=np.random.choice(action_space)\n",
    "        return np.random.choice([random_action,greedy_action],p=[epsilon,1-epsilon]) \n",
    "    return epsilon_greedy_policy\n",
    "def sarsa(env, gamma, n_episode, alpha, epsilon):\n",
    "    \n",
    "    length_episode = np.zeros(n_episode) \n",
    "    sum_reward_episode = np.zeros(n_episode)\n",
    "    \n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.zeros(len(env.action_space)))))\n",
    "    \n",
    "    pi = gen_epsilon_greedy_policy(env.action_space, Q, epsilon)\n",
    "   \n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        \n",
    "        ## Start Coding Here\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        action = pi(state)\n",
    "        while not is_done:\n",
    "            next_state, reward, is_done = env.step(action)\n",
    "            next_action = pi(next_state)\n",
    "            td_delta = reward + gamma * Q[next_state][next_action] - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "            \n",
    "            length_episode[episode] += 1\n",
    "            sum_reward_episode[episode] += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "        ## End Coding Here\n",
    "            \n",
    "    return Q,pi\n",
    "# Save the policy for alpha=0.4 in a Python function pi_sarsa_1\n",
    "# Save the policy for alpha=0.1 in a Python function pi_sarsa_2\n",
    "n_episode=50000\n",
    "epsilon=0.1\n",
    "alpha=0.4\n",
    "gamma=1\n",
    "Q,pi_sarsa_1=sarsa(env, gamma, n_episode, alpha, epsilon)\n",
    "alpha=0.1\n",
    "Q_b,pi_sarsa_2=sarsa(env, gamma, n_episode, alpha, epsilon)\n",
    "# End Coding Here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 63/10000 [00:00<00:38, 259.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  Sarsa, alpha=0.4  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:28<00:00, 349.00it/s]\n",
      "  2%|▏         | 206/10000 [00:00<00:09, 1037.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: -36.84618461846185\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0001\n",
      "--------  Sarsa, alpha=0.1  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1040.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 2.3004\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from env_taxi import performance_evaluation\n",
    "\n",
    "print('--------  Sarsa, alpha=0.4  --------')\n",
    "performance_evaluation(env,pi_sarsa_1)\n",
    "\n",
    "print('--------  Sarsa, alpha=0.1  --------')\n",
    "performance_evaluation(env,pi_sarsa_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare their performance, and comment on the similarties or/and differences with that of Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:When using these two Sarsa methods, an episode could be very likely to terminate within 1000 steps,but alpha=0.4，there is an very small probability that an episode can not terminate with 1000 steps .Also the different alpha leads to quite different sum of rewards.Compared with Q-learning methods, these two sum of rewards are not statisfying.And the choice of alpha seems have a great effect on the results of Sarsa,while for Q-learning, alpha could change nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Do you expect double Q-learning will improve Q-learning substantially? Motivate your answer. You do not need to implement the double Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:Yes,we do.Using double Q-learning,we could expect that the effect of extreme action value will decrease,and the estimation error will decrease，which means eliminating the maximization bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** \n",
    "Follow the steps below to investigate whether we can solve the optimal policy by using off-policy Monte Carlo control with weighted importance sampling. Use the optimal $\\varepsilon$-soft policy found by Sarsa above with $\\alpha=0.1$ as the behavior policy and use no more than 50000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:51<00:00, 971.94it/s] \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(GroupNumber)\n",
    "\n",
    "## Start Coding Here\n",
    "def mc_control_off_policy_weighted(env, gamma, n_episode):\n",
    "    \n",
    "    n_action = len(env.action_space)\n",
    "    C = defaultdict(float)\n",
    "    \n",
    "    # Create a function to generate action based on the behavior policy\n",
    "    b_func=pi_sarsa_2\n",
    "    \n",
    "    # Initialization\n",
    "    Q = defaultdict(lambda: dict(zip(env.action_space, np.empty(6))))\n",
    "    pi= defaultdict(lambda: np.random.choice(env.action_space))\n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        states_t, actions_t, rewards_t = simulate_episode(env, b_func)\n",
    "        \n",
    "        ## Start Coding Here\n",
    "        W = 1.\n",
    "        return_t = 0.\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1][1:], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            C[(state_t, action_t)] += W\n",
    "            Q[state_t][action_t] += (W / C[(state_t, action_t)]) * (return_t - Q[state_t][action_t])\n",
    "            pi[state_t]=max(Q[state_t],key=Q[state_t].get)\n",
    "            \n",
    "            if action_t != pi[state_t]:\n",
    "                break\n",
    "            if action_t in max(Q[state_t],key=Q[state].get):\n",
    "                b=((1-epsilon)/len(max(Q[state_t],key=Q[state].get))+epsilon/len(env.action_space))\n",
    "            else:\n",
    "                b=epsilon/len(env.action_space)\n",
    "            W *= 1./ b\n",
    "        ## End Coding Here\n",
    "                \n",
    "    return  pi\n",
    "# Save your optimal policy as a dictionary/defaultdict Python object pi_wis\n",
    "n_episode=50000\n",
    "\n",
    "\n",
    "pi_wis=mc_control_off_policy_weighted(env, gamma, n_episode)\n",
    "\n",
    "\n",
    "## End Coding Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total number of distinct states visited in your episodes. Have you reached all 400 possible states (within the episode)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States Visited: 354\n"
     ]
    }
   ],
   "source": [
    "print('Number of States Visited:', len(pi_wis)) #less than 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a complete policy that:\n",
    "- follows your optimal policy (in this exercise) if the state is visited in your episodes (in this exercise);\n",
    "- otherwise follows the naive policy in Exercise 1.\n",
    "\n",
    "If you have reached all possible states in your episodes, it is identical to your optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_policy(state):\n",
    "    ## Start Coding Here\n",
    "    if state in pi_wis:\n",
    "        action=pi_wis[state]\n",
    "    else:\n",
    "        action=naive_policy(state) \n",
    "    ## End Coding Here\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2067.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rewards per episode: 6.8081\n",
      "The percentage of episodes that cannot terminate within 1000 steps: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "performance_evaluation(env,complete_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: What are your conclusions from the analysis above? What are the advantages and disadvantages of these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:From all the policy evaluations, we can conclude that Q-learning has the highest reward among all methods. While Monte Carlo Exploring Starts gives the worst reward which more than half of episodes can not end within 1000 steps. Q-learing is the most informative method for this particular question. However, Q-learning takes longer time since the length per episode is longer than length of SARSA. And SARSA gives a lower rewards but gives a faster route.Using off-policy Monte Carlo control with weighted importance sampling and combined with the naive policy could also get an satisfactory result,and it takes a short time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "11b798d223644fa9683b216202a0404c1de1b25edb54c0e0e7003e20cdf3c2c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
